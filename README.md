# ğŸ§ Emotion Classification from Audio using Wav2Vec2 and Deep Learning

## ğŸ“ Project Description

This project focuses on **classifying emotions from audio** using the **RAVDESS (Ryerson Audio-Visual Database of Emotional Speech and Song)** dataset. We implement and compare a variety of classical and deep learning techniques, and finally fine-tune the **Wav2Vec2** model to achieve high performance.
The project is live on Netlify(https://emotion-classification-ravdess.streamlit.app/).

---

## ğŸ“‚ Dataset

### RAVDESS (Ryerson Audio-Visual Database of Emotional Speech and Song)

- Audio-only `.wav` files named in the format:  
  `03-01-06-01-02-01-12.wav`  
  Each filename contains metadata including **modality, vocal channel, emotion, intensity, statement, repetition**, and **actor ID**.

- Emotion Code Mapping:
01 = neutral
02 = calm
03 = happy
04 = sad
05 = angry
06 = fearful
07 = disgust
08 = surprised

  
---

## ğŸ§ª Preprocessing

- Convert stereo to mono
- Resample to 16kHz
- Pad/truncate to fixed length (4 seconds)
- Extract features using `librosa`:
- **MFCCs**, **Chroma**, **ZCR**, **Spectral Contrast**
- Global statistics like **loudness**, **RMS**, **SNR**, etc.

---

## ğŸ§  Model Pipeline

We experimented with the following models:

| Model               | Accuracy |
|---------------------|----------|
| Random Forest       | 74%      |
| CNN                 | 80%      |
| CNN + LSTM          | 82%      |
| CNN + GRU           | 83%      |
| **Fine-tuned Wav2Vec2** | **89%** |

### âœ… Final Model: Wav2Vec2

- Pretrained model: `facebook/wav2vec2-base`
- Fine-tuned for multi-class emotion classification
- Training:
- Epochs: 12
- Batch size: 1
- Learning rate: 1e-5
- Evaluation metric: weighted F1 score
- Load best model based on F1

---

## ğŸ“ˆ Evaluation Metrics

Final evaluation on RAVDESS validation set:

- **Accuracy**: 89%
- **Weighted F1-Score**: 88.6%
- **Per-class metrics**: Provided in classification report
- **Confusion Matrix**: Created with classification report

---

## ğŸ”„ Inference

To test the model on your own `.wav` files (following RAVDESS filename structure), run:

```bash
python test_emotion_model.py
```

## Folder Structure

```
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ speech_actors/
â”‚   â”œâ”€â”€ song_actors/
â”œâ”€â”€ saved_model/
â”‚   â””â”€â”€ emotion_wav2vec2/
â”œâ”€â”€ test.csv
â”œâ”€â”€ test_emotion_model.py
â”œâ”€â”€ app.py
â””â”€â”€ README.md
```

## Future Work

 - Ensemble multiple deep learning models

 - Add attention mechanism to BiLSTM layers

 - Extend to multilingual or multi-modal emotion datasets
